{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 32-bit",
   "metadata": {
    "interpreter": {
     "hash": "21e7278c2c4ff98f40814feed63f8e68cb2f53812443804446a63ed1f8387ee9"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Python Scientific Data Analysis\n",
    "## Course's Final Project\n",
    "### Barak Daniel - 204594329"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Installations needed for the program to run:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install seaborn\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install sklearn\n",
    "!pip install scipy\n",
    "!pip install pydotplus\n",
    "!conda install -y python-graphviz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import sklearn as skl\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import pydotplus\n",
    "import random\n",
    "from sklearn import tree\n",
    "from io import StringIO\n",
    "from IPython.display import Image\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "source": [
    "# Intro\n",
    "\n",
    "### Overview\n",
    "This project is about targetted marketing, in the data set given we have 'customer' as each row of data, and the features can tell us about the customer's status in general (age, marriage, etc..) and hes shopping behavior.\n",
    "In this section of the project I will go over the data set and the goal to try and understand the whole process before starting the actual work on the data.\n",
    "\n",
    "### So what exactly is targetted marketing?\n",
    "Targeting in marketing is a strategy that breaks a large market into smaller segments to concentrate on a specific group of customers within that audience. It defines a segment of customers based on their unique characteristics and focuses solely on serving them.\n",
    "Instead of trying to reach an entire market, a brand uses target marketing to put their energy into connecting with a specific, defined group within that market.\n",
    "So for this reason I'll break down all the features in the given dataset ('customers3.csv'), and understand them.\n",
    "\n",
    "\n",
    "### Feature's data breadown\n",
    "The following features are included in the data set given in 'customes3.csv':\n",
    "- ID - Unique ID to each customer\n",
    "- Gender - The gender of the customer\n",
    "- Ever_Marries - Indicates if the customer was married\n",
    "- Age - The age of the customer\n",
    "- Graduated - Has the customer graduated high school\n",
    "- Profession - The profession of the customer\n",
    "- Work_Experience - The number of years of the customer's expirence in his profession\n",
    "- Spending_Score - The spending habits of the customer classified to 3 categories\n",
    "- Family_Size - The number of family members the customer has in his household\n",
    "- Shop_Day - The day of the week which the customer is shopping on the most\n",
    "- Shop_Other - Normalized measure of customer deviation from average store customer spending on non specified products\n",
    "- Shop_Dairy - Normalized measure of customer deviation from average store customer spending on dairy products\n",
    "- Shop_Household: Normalized measure of customer deviation from average store customer spending on household products\n",
    "- Shop_Meat - Normalized measure of customer deviation from average store customer spending on meat products\n",
    "- Group - The target group which the customer belongs to\n",
    "\n",
    "### Feature's type breakdown\n",
    "- ID - Numerical discrete (Integer)\n",
    "- Gender - Categorical (Male/Female)\n",
    "- Ever_Marries - Categorical nominal (Yes/No)\n",
    "- Age - Numerical continuous (Integer)\n",
    "- Graduated - Categorical (Yes/No)\n",
    "- Profession - Categorical nominal\n",
    "- Work_Experience - Numerical discrete (Integer)\n",
    "- Spending_Score - Categorical ordinal (Low/average/High)\n",
    "- Family_Size - Numerical discrete (Integer)\n",
    "- Shop_Day - Categorical ordinal (Sunday, Monday, ..., Saturday)\n",
    "- Shop_Other - Numerical continuous (Double)\n",
    "- Shop_Dair - Numerical continuous (Double)\n",
    "- Shop_Household - Numerical continuous (Double)\n",
    "- Shop_Meat -Numerical continuous (Double)\n",
    "- Group - Categorical nominal"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('customers3.csv')\n",
    "count = df.count()\n",
    "\n",
    "print(\"The number of rows is: {}\".format(len(df.index)))\n",
    "print(\"The number of columns is: {}\".format(len(df.columns)))\n",
    "print(\"The number of cells is: {}\".format(len(df.index) * len(df.columns)))\n",
    "print(\"The number of cells with concrete values is: {}\".format(count.sum()))\n",
    "print(\"The number of cells without concrete values is: {}\\n\".format(len(df.index) * len(df.columns) - count.sum()))\n",
    "\n",
    "print(\"\\nThe number of concrete values for each feature:\")\n",
    "df.count()\n"
   ]
  },
  {
   "source": [
    "### The size of the data set is:\n",
    "- 8120 rows of customer's data (+1 for the headers of each column)\n",
    "- 15 columns for the features\n",
    "- 8120*15 = 121,8000 cells, but we can see that not all of them has concrete values.\n",
    "\n",
    "### Missing values:\n",
    "After watching the dataset and trying to understand it, I have also encountered many cells with missing data values.\n",
    "After reading the dataset a transformation of this Nan values is needed, for each feature with missing data, I'll examine it and understand which of the methods is the best to deal with those values (Mode, Mean, Median, Removal, etc..).\n",
    "\n",
    "### Other types of missing values:\n",
    "A validation for the values that are not missing must be made, after going through the features, the options are numeric value which is out of the range as given with feature definition, a numeric value that cannot be negative, etc...\n",
    "\n",
    "After going through out the dataset, those are the features needed to be fixed:\n",
    "- Shop_Day - Must contain values of 1 to 7 but there are values out of this range therefore it will be filled by the same method as all the feature values\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Initial Data Analysis\n",
    "\n",
    "As we saw above, there are a lot of missing values and categorical values we want to transform before we can alanyze the data completely.\n",
    "In this section of the project I will deal with those values, for each feature a check for the accuracy of the model will be taken and by that I can make the decision what was the best method for the feature."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillMissingValues(df, target, useRound=0):\n",
    "    group_list = [\"A\", \"B\", \"C\", \"D\"]\n",
    "    df[\"Group_Transformed\"] = pd.Categorical(df.Group, ordered=True, categories=group_list).codes + 1\n",
    "\n",
    "    print(\"The correlation before filling the data is: \",df[\"Group_Transformed\"].corr(df[target]))\n",
    "\n",
    "    aggMin = df[target].min()\n",
    "    aggMax = df[target].max()\n",
    "    if(useRound == 0):\n",
    "        aggMean = df[target].mean()\n",
    "    else:\n",
    "        aggMean = round(df[target].mean())\n",
    "    aggMedian = df[target].median()\n",
    "    aggMode = df[target].mode()[0]\n",
    "\n",
    "    print(\"\\n{} aggregations:\\nMean = {}\\nMedian = {}\\nMode = {}\".format(targer ,aggMean, aggMedian, aggMode))\n",
    "    print(\"Min = {}  --- Max = {}\\n\".format(aggMin, aggMax))\n",
    "\n",
    "    targetMean = target+\"_mean\"\n",
    "    targetMedian = target+\"_median\"\n",
    "    targetMode = target+\"_mode\"\n",
    "\n",
    "    df[targetMean] = df[target].fillna(aggMean)\n",
    "    df[targetMedian] = df[target].fillna(aggMedian)\n",
    "    df[targetMode] = df[target].fillna(aggMode)\n",
    "\n",
    "    corrMean = df[\"Group_Transformed\"].corr(df[targetMean])\n",
    "    corrMedian =  df[\"Group_Transformed\"].corr(df[targetMedian])\n",
    "    corrMode = df[\"Group_Transformed\"].corr(df[targetMode])\n",
    "\n",
    "    print(\"Corr with mean: \", corrMean)\n",
    "    print(\"Corr with median: \", corrMedian)\n",
    "    print(\"Corr with mode: \", corrMode)\n",
    "\n",
    "    if(corrMean > corrMedian):\n",
    "        if(corrMean > corrMode):\n",
    "            df[target] = df[targetMean]\n",
    "        else:\n",
    "            df[target] = df[targetMode]\n",
    "    elif(corrMedian > corrMode):\n",
    "        df[target] = df[targetMedian]\n",
    "    else:\n",
    "        df[target] = df[targetMode]\n",
    "\n",
    "    df = df.drop(['Group_Transformed', targetMean, targetMedian, targetMode], axis=1, inplace=True)"
   ]
  },
  {
   "source": [
    "The first feature to handle missing data will be 'Gender', since we have less than 60 missing values and its binary the best method to do it is to check their distributions over the data set and then fill them with this distributions.\n"
   ],
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male = 0\n",
    "female = 0\n",
    "for index,row in df.iterrows():\n",
    "    if(row['Gender'] == 'Male'):\n",
    "        male += 1\n",
    "    elif(row['Gender'] == 'Female'):\n",
    "        female += 1\n",
    "\n",
    "print(\"Female precentage\", female/(male+female))\n",
    "print(\"Male precentage\", male/(male+female))"
   ]
  },
  {
   "source": [
    "So we can see now that females represents ~45.25% of the rows in the data set and Males are ~54.75% .\n",
    "Now we fill the missing values with this distribution:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nans = df['Gender'].isna()\n",
    "length = sum(nans)\n",
    "replacement = random.choices(['Male', 'Female'], weights=[.5475, .4525], k=length)\n",
    "df.loc[nans,'Gender'] = replacement\n",
    "\n",
    "df.Gender.count()"
   ]
  },
  {
   "source": [
    "The next feature I'll be dealing with is Ever_Married which is also a binary answer of yes or no.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "married = 0\n",
    "notmarried = 0\n",
    "for index,row in df.iterrows():\n",
    "    if(row['Ever_Married'] == 'Yes'):\n",
    "        married += 1\n",
    "    elif(row['Ever_Married'] == 'No'):\n",
    "        notmarried += 1\n",
    "\n",
    "\n",
    "married_list = [\"Yes\", \"No\"]\n",
    "df[\"Ever_Married_Transformed\"] = pd.Categorical(df.Ever_Married, ordered=True, categories=married_list).codes + 1\n",
    "group_list = [\"A\", \"B\", \"C\", \"D\"]\n",
    "df[\"Group_Transformed\"] = pd.Categorical(df.Group, ordered=True, categories=group_list).codes + 1\n",
    "\n",
    "print(df[\"Group_Transformed\"].corr(df[\"Ever_Married_Transformed\"]))\n",
    "df.drop(['Ever_Married_Transformed', 'Group_Transformed'], axis=1)\n",
    "\n",
    "print(\"Percentage of married = \", (married/(married+notmarried)))\n",
    "print(\"Percentage of married = \", (notmarried/(married+notmarried)))\n",
    "\n",
    "df = df.drop([\"Ever_Married_Transformed\"], axis=1)"
   ]
  },
  {
   "source": [
    "The correlation to the group is low and therefore I can use the same method as I did in 'Gender'."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nans = df['Ever_Married'].isna()\n",
    "length = sum(nans)\n",
    "replacement = random.choices(['Yes', 'No'], weights=[.5859, .4141], k=length)\n",
    "df.loc[nans,'Ever_Married'] = replacement\n",
    "\n",
    "df.Ever_Married.count()"
   ]
  },
  {
   "source": [
    "The next feature to deal with is 'Age', this feature fits the fillMissingValues() function I created above, therefore I'll use it here."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fillMissingValues(df, \"Age\")\n",
    "df[\"Age\"].count()"
   ]
  },
  {
   "source": [
    "As we can see, both of the values are nearly the same, but we will still prefer the option for the better correlation even if it is only slightly higher."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "The next feature will be Graduated which missing a few values, so like the other binary features I have dealt with above, I'll do the same here."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = 0\n",
    "ungrad = 0\n",
    "for index,row in df.iterrows():\n",
    "    if(row['Graduated'] == 'Yes'):\n",
    "        grad += 1\n",
    "    elif(row['Graduated'] == 'No'):\n",
    "        ungrad += 1\n",
    "\n",
    "print(\"Graduated precentage\", ungrad/(grad+ungrad))\n",
    "print(\"Haven't graduated precentage\", grad/(grad+ungrad))\n",
    "\n",
    "nans = df['Graduated'].isna()\n",
    "length = sum(nans)\n",
    "replacement = random.choices(['Yes', 'No'], weights=[.3781, .6219], k=length)\n",
    "df.loc[nans,'Graduated'] = replacement\n",
    "\n",
    "df.Graduated.count()"
   ]
  },
  {
   "source": [
    "The next feature is Profession, the difference from the features we dealt with already is that this feature is Categorical nominal which the set of his values is not finite, therefore to fill this column and not lose the data from the rest of the rows, I will fill the values with Mode."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proMode = df.Profession.mode()[0]\n",
    "df[\"Profession\"] = df.Profession.fillna(proMode)"
   ]
  },
  {
   "source": [
    "Now the feature to be dealt with is Work_Expirence, because there is the Age col, maybe here we can fill the missing values with deductive imputation, so now I'll check their corr and decide how to fill this feature."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Age.corr(df.Work_Experience)"
   ]
  },
  {
   "source": [
    "Because the corr is low, using the Age feature won't be good enough for filling those values, now I'll check the mathematical operation that can be done."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fillMissingValues(df, \"Work_Experience\")\n",
    "df[\"Work_Experience\"].count()"
   ]
  },
  {
   "source": [
    "As we can see the mean option for filling the values is the best we can get here and is close to the original correlation before filling the missing data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fillMissingValues(df, \"Family_Size\", 1)"
   ]
  },
  {
   "source": [
    "Both mean and median gave the same result and got better correlation than mode, that is why their value will be used for filling the missing values."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "For the next feature, \"Shop_Day\", before filling the missing values I need to deal with the wrong values the feaure is containing:\n",
    "Must contain values of 1 to 7 but there are values out of this range."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.Shop_Day.unique())"
   ]
  },
  {
   "source": [
    "The values that needs to be dealt with first are 0 and 22"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_Shop_Day = df.Shop_Day\n",
    "for index, value in enumerate(temp_Shop_Day):\n",
    "    if(value == 0 or value == 22):\n",
    "        temp_Shop_Day[index] = np.nan\n",
    "\n",
    "df.Shop_Day = temp_Shop_Day\n",
    "print(df.Shop_Day.unique())"
   ]
  },
  {
   "source": [
    "Now I can fill the missing values by checking the best mathematical operation without wrong values."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fillMissingValues(df, \"Shop_Day\", useRound=1)\n",
    "df[\"Shop_Day\"].count()"
   ]
  },
  {
   "source": [
    "For the Shop_Day filling options we can see that the mean is the better option here and very close to the original correlation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Moving on to the next feature, \"Shop_Diary\", I will check all the fitting mathematical operations as well."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fillMissingValues(df, \"Shop_Dairy\")\n",
    "df[\"Shop_Dairy\"].count()"
   ]
  },
  {
   "source": [
    "The mean and median are pretty close in their correlation to the group, still mean is higher so that is why it will be used to fill the values. \n",
    "\n",
    "For the next 2 feature's, \"Shop_Household\" and \"Shop_Meat\", I will use the same method as in Dairy "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fillMissingValues(df, \"Shop_Household\")\n",
    "df[\"Shop_Household\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fillMissingValues(df, \"Shop_Meat\")\n",
    "df[\"Shop_Meat\"].count()"
   ]
  },
  {
   "source": [
    "Both \"Shop_Household\" and \"Shop_Meat\" will get the best result by filling with the mean value of the feature."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Now that all the dataset is fixed I'll save it to a new csv."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.count())\n",
    "df.to_csv(\"customer3_fixed.csv\", index=False)"
   ]
  },
  {
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "feature correlation:\n",
    "- Turning categorical features to numeric representation\n",
    "- Features histograma\n",
    "- Ploting correlation map"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_list = [\"Male\", \"Female\"]\n",
    "df[\"Gender\"] = pd.Categorical(df.Gender, ordered=True, categories=group_list).codes + 1\n",
    "\n",
    "group_list = [\"Yes\", \"Now\"]\n",
    "df[\"Ever_Married\"] = pd.Categorical(df.Ever_Married, ordered=True, categories=group_list).codes + 1\n",
    "\n",
    "group_list = [\"Yes\", \"Now\"]\n",
    "df[\"Graduated\"] = pd.Categorical(df.Graduated, ordered=True, categories=group_list).codes + 1\n",
    "\n",
    "group_list = []\n",
    "uniqueProf = df.Profession.unique()\n",
    "for prof in uniqueProf:\n",
    "    group_list.append(prof)\n",
    "\n",
    "df[\"Profession\"] = pd.Categorical(df.Profession, ordered=True, categories=group_list).codes + 1\n",
    "\n",
    "group_list = [\"Low\", \"Average\", \"High\"]\n",
    "df[\"Spending_Score\"] = pd.Categorical(df.Spending_Score, ordered=True, categories=group_list).codes + 1\n",
    "\n",
    "group_list = [\"A\", \"B\", \"C\", \"D\"]\n",
    "df[\"Group\"] = pd.Categorical(df.Group, ordered=True, categories=group_list).codes + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = df.hist(bins=20, figsize=(20,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr()\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "f, ax = plt.subplots(figsize=(21, 17))\n",
    "cmap = sns.diverging_palette(200, 10, as_cmap=True)\n",
    "sns.heatmap(corr, mask=mask, cmap = cmap, center=0, annot = True , square = True , linewidths = .6, vmin = -1, vmax = 1 )\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "What can we learn from the correlation heatmap?\n",
    "\n",
    "- ID: is just the id of the observation therefore there shouldn't be any relation for it to the other features.\n",
    "\n",
    "- Gender: this feature is not in a good correlation with Group, it can't help us learn much.\n",
    "\n",
    "- Ever_Married: comparing to the other values, there is a small negative correlation to Group which indicates that people who got ever married mostly got into an higher group.\n",
    "\n",
    "- Age: this is the 3rd best correlation to Group we can see in the dataset, this correlation can tell us that the younger the customer, the more likley he will be in a higher group.\n",
    "\n",
    "- Graduated: this feature's correlation to Group is average comparing to the other features and this small correlation\n",
    " indicates that people who did graduate will be more likeley to be in an higher group.\n",
    " \n",
    "- Profession: this feature is also on an average correlation to Group comparing to the other features, but we can't learn much from its correlation to Group since it is a nominal feature and can't really indicate a behavior.\n",
    "\n",
    "- Work_Expirence: The lowest correlation to Group comparing to all the other features, we can't learn from it anything relating to the target group.\n",
    "\n",
    "- Spending_Score: this feature is not so correlated to the Group target feature, but is still has some negative correlation, which indicates that the lower the shoping score of the customer the more likeley he will be in an higher group.\n",
    "\n",
    "- Family_Size: this feature has an average correlation to Group and we can learn from it that as the family size grow bigger, there is more chance to be in an higher group.\n",
    "\n",
    "- Shop_Day: the favorite shop day of the week as a very small correlation to Group and therefore we can't learn much from it.\n",
    "\n",
    "- Shop_Other: the 2nd best feature for correlation with Group, we learn from it that the more the customer is spending on other things than dairy, meat and household, the higher group he will probably be in.\n",
    "\n",
    "- Shop_Dairy: an average correlation compared to the other features but we can still learn that the more the customer spend on dairy the more likley he will be in an higher group.\n",
    "\n",
    "- Shop_Household: this feature has the best correlation to group in all the dataset, we can learn that the more the customer spends on household he will probably get into an higher group.\n",
    "\n",
    "- Shop_Meat: this feature has an average correlation comped to the other features and indicates a small connection between the amount a customer spend on meat and the group as the more he spend the higher group he will probably get into.\n",
    "\n",
    "Addtional insights of correlation not related to the target feature:\n",
    "- Age and Ever_Married has a strong correlation, this makes sense of course, I was wondering if I should remove one of them because of their high correlation but I decided that at this stage I will keep them both.\n",
    "\n",
    "- Spending_Score has a high correlation with Age and Ever_Married, which are correlated as explained above, but here again I don't find a need to drop one of the features because the correlation isn't high enough and the data is not similliar."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Now I'll display the most relevant features in graphs:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = df['Group']\n",
    "features = ['Shop_Household', 'Shop_Other', 'Age']\n",
    "\n",
    "for feat in features:\n",
    "    temp  = df[feat]\n",
    "    cor = group.corr(temp)\n",
    "    plt.title('Group {} correlation {}'.format(feat,cor))\n",
    "    plt.xlabel(feat)\n",
    "    plt.ylabel('Group')\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(8, 6)\n",
    "    plt.scatter(temp, group)\n",
    "    plt.plot(np.unique(temp), np.poly1d(np.polyfit(temp, group, 1))(np.unique(temp)), color='red', linewidth = 2.9)\n",
    "    plt.show()"
   ]
  },
  {
   "source": [
    "From the 3 graphs above we can see clearly that there is a linear correlation between each of these features to the Group feature.\n",
    "now I will show those features in a pair plot to see if there is more insights about their data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairPlotDf = df[['Shop_Household', 'Shop_Other', 'Age', 'Group']]\n",
    "sns.set_context(rc={\"axes.labelsize\":20})\n",
    "pairPlot = sns.pairplot(pairPlotDf, hue='Group', palette='Set1', corner=True)\n",
    "pairPlot.fig.set_size_inches(20, 20)\n",
    "pairPlot._legend.remove()\n",
    "plt.legend(title='Group', loc=(2., 1.5), labels=['A', 'B', 'C', 'D'], prop={'size': 18}, title_fontsize='21')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "We can see that as the heatmap showed us that the most correlated features to the Group target feature are giving also the most clrear classification.\n",
    "In the pairplot above the Shop_Other and Shop_Household giving the best result for the scatter of the groups.\n",
    "\n",
    "The last feature I want to check in this part is the Profession because it is hard to know anything from it by looking at the correlation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairPlotDf = df[['Shop_Household', 'Shop_Other', 'Profession', 'Group']]\n",
    "sns.set_context(rc={\"axes.labelsize\":20})\n",
    "pairPlot = sns.pairplot(pairPlotDf, hue='Group', palette='Set1', corner=True)\n",
    "pairPlot.fig.set_size_inches(20, 20)\n",
    "pairPlot._legend.remove()\n",
    "plt.legend(title='Group', loc=(2., 1.5), labels=['A', 'B', 'C', 'D'], prop={'size': 18}, title_fontsize='21')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "Now we can realy see that both the heatmap and the pairplot indicates that the Profession feature is not correlated to the Group feature, and since it is not related in a strong way to any other feature, there is no information we can get from it in this stage."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Classification Model\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Task 1\n",
    "The way I'll see which are the best features for my model, is that I'll partice my model with 2 different features everytime and decide by that."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"Gender\", \"Ever_Married\", \"Age\", \"Graduated\", \"Work_Experience\", \"Spending_Score\", \"Family_Size\", \"Shop_Day\", \"Shop_Other\", \"Shop_Dairy\", \"Shop_Household\", \"Shop_Meat\"]\n",
    "bestFeats = []\n",
    "bestScore = 0\n",
    "\n",
    "for i in range(len(features)):\n",
    "    for j in range(len(features)):\n",
    "        if(i != j):\n",
    "            x_data = df[[features[i], features[j]]]\n",
    "            y_group = df.Group\n",
    "            x_train, x_test, y_train, y_test =  train_test_split(x_data, y_group, train_size= 0.8, random_state=1)\n",
    "    \n",
    "            from sklearn.naive_bayes import GaussianNB\n",
    "            model = GaussianNB()                       \n",
    "            prediction = model.fit(x_train, y_train)                  \n",
    "            y_model = model.predict(x_test)\n",
    "\n",
    "            if(bestScore < metrics.accuracy_score(y_test, y_model)):\n",
    "                bestScore = metrics.accuracy_score(y_test, y_model)\n",
    "                bestFeats = [features[i], features[j]]\n",
    "                \n",
    "print(bestScore, bestFeats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_group = df.Group\n",
    "for index,val in enumerate(temp_group):\n",
    "    if(val == 1):\n",
    "        temp_group[index] = \"A\"\n",
    "    elif(val == 2):\n",
    "        temp_group[index] = \"B\"\n",
    "    elif(val == 3):\n",
    "        temp_group[index] = \"C\"\n",
    "    elif(val == 4):\n",
    "        temp_group[index] = \"D\"\n",
    "df.Group = temp_group\n",
    "y = df.Group\n",
    "X = df[['Shop_Meat', 'Shop_Other']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=1)\n",
    "\n",
    "clf = GaussianNB()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the response for test dataset\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "metrics.classification_report(y_test, y_pred)\n",
    "\n",
    "hueorder = clf.classes_\n",
    "\n",
    "x_min, x_max = X_train.loc[:, 'Shop_Meat'].min() -1, X_train.loc[:, 'Shop_Meat'].max() +1\n",
    "y_min, y_max = X_train.loc[:, 'Shop_Other'].min() -1, X_train.loc[:, 'Shop_Other'].max() +1\n",
    "\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.2), np.arange(y_min, y_max, 0.2))\n",
    "\n",
    "Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = np.argmax(Z, axis=1)\n",
    "\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, cmap='Set1', alpha=0.5)\n",
    "plt.clim(0, len(clf.classes_)+3)\n",
    "sns.scatterplot(data=df[::5], hue='Group', hue_order=hueorder, palette='Set1', x='Shop_Meat', y='Shop_Other')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "As we can see above, the classification gives a pretty good classification for A and B values in the Group targer feature, but we can still see the classification of values C and D, although it is not as accurate.\n",
    "\n",
    "To see farther information I will display all the prediction which didn't work."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = df.Group\n",
    "X = df[['Shop_Meat', 'Shop_Other']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=1)\n",
    "\n",
    "clf = GaussianNB()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the response for test dataset\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "metrics.classification_report(y_test, y_pred)\n",
    "\n",
    "hueorder = clf.classes_\n",
    "\n",
    "x_min, x_max = X_train.loc[:, 'Shop_Meat'].min() -1, X_train.loc[:, 'Shop_Meat'].max() +1\n",
    "y_min, y_max = X_train.loc[:, 'Shop_Other'].min() -1, X_train.loc[:, 'Shop_Other'].max() +1\n",
    "\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.2), np.arange(y_min, y_max, 0.2))\n",
    "\n",
    "Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = np.argmax(Z, axis=1)\n",
    "\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, cmap='Set1', alpha=0.5)\n",
    "plt.clim(0, len(clf.classes_)+3)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 10)\n",
    "plt.xlabel('Shop_Meat')\n",
    "plt.ylabel('Shop_Other')\n",
    "\n",
    "\n",
    "#Adding overlay:\n",
    "x_data = df[['Shop_Meat', 'Shop_Other']]\n",
    "\n",
    "y_pred = clf.predict(x_data)\n",
    "\n",
    "incorrect_vals = []\n",
    "temp_group = df.Group\n",
    "temp_vals = df.values.tolist()\n",
    "\n",
    "for i in range(len(df)):\n",
    "    if(y_pred[i] != temp_vals[i][14]):\n",
    "        incorrect_vals.append(temp_vals[i])\n",
    "\n",
    "incorrect_vals = pd.DataFrame(incorrect_vals, columns=df.columns)\n",
    "\n",
    "temp_group = clf.classes_\n",
    "\n",
    "sns.scatterplot(data=incorrect_vals[::5], hue='Group', hue_order=temp_group, palette='Set1', x='Shop_Meat', y='Shop_Other')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "### Task 2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basicDf = pd.read_csv(\"customers3.csv\")\n",
    "basicDf = basicDf.dropna()\n",
    "\n",
    "group_list = [\"Male\", \"Female\"]\n",
    "basicDf[\"Gender\"] = pd.Categorical(basicDf.Gender, ordered=True, categories=group_list).codes + 1\n",
    "\n",
    "group_list = [\"Yes\", \"Now\"]\n",
    "basicDf[\"Ever_Married\"] = pd.Categorical(basicDf.Ever_Married, ordered=True, categories=group_list).codes + 1\n",
    "\n",
    "group_list = [\"Yes\", \"Now\"]\n",
    "basicDf[\"Graduated\"] = pd.Categorical(basicDf.Graduated, ordered=True, categories=group_list).codes + 1\n",
    "\n",
    "group_list = []\n",
    "uniqueProf = basicDf.Profession.unique()\n",
    "for prof in uniqueProf:\n",
    "    group_list.append(prof)\n",
    "\n",
    "basicDf[\"Profession\"] = pd.Categorical(basicDf.Profession, ordered=True, categories=group_list).codes + 1\n",
    "\n",
    "group_list = [\"Low\", \"Average\", \"High\"]\n",
    "basicDf[\"Spending_Score\"] = pd.Categorical(basicDf.Spending_Score, ordered=True, categories=group_list).codes + 1\n",
    "\n",
    "group_list = [\"A\", \"B\", \"C\", \"D\"]\n",
    "basicDf[\"Group\"] = pd.Categorical(basicDf.Group, ordered=True, categories=group_list).codes + 1\n",
    "\n",
    "x_data = basicDf.drop(\"Group\", axis=1)\n",
    "y_group = basicDf.Group\n",
    "x_train, x_test, y_train, y_test =  train_test_split(x_data, y_group, train_size= 0.8, random_state=1)\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = tree.DecisionTreeClassifier(max_depth=7)               \n",
    "prediction = model.fit(x_train, y_train)                  \n",
    "y_model = model.predict(x_test)\n",
    "print(metrics.accuracy_score(y_test, y_model))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = df.drop([\"Group\", \"Gender\", \"ID\", \"Ever_Married\"], axis=1)\n",
    "y_group = df.Group\n",
    "x_train, x_test, y_train, y_test =  train_test_split(x_data, y_group, train_size= 0.8, random_state=1)\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = tree.DecisionTreeClassifier(max_depth=7)               \n",
    "prediction = model.fit(x_train, y_train)                  \n",
    "y_model = model.predict(x_test)\n",
    "print(metrics.accuracy_score(y_test, y_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting importances\n",
    "result = permutation_importance(model, x_data, y_group, n_repeats=10, random_state=0)\n",
    "importance = zip(x_data.columns, result['importances_mean'])\n",
    "\n",
    "# summarize feature importance\n",
    "for i,v in importance:\n",
    "    print(\"Feature: {}, Score: {}\".format(i,v))\n",
    "\n",
    "# plot feature importance\n",
    "plt.bar(range(len(x_data.columns)), result['importances_mean'])\n",
    "plt.xticks(ticks=range(len(x_data.columns)), labels=x_data.columns, rotation=90)\n",
    "plt.show()\n",
    "\n",
    "dot_data = StringIO()\n",
    "tree.export_graphviz(model, out_file=dot_data, filled=True, rounded=True, feature_names=x_data.columns, class_names=model.classes_)\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "graph.write_png('Customers.png')\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}